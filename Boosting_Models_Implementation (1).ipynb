{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75baf52d",
   "metadata": {},
   "source": [
    "Q.14 Train an AdaBoost Classifier on a sample dataset and print model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da201105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8e47e",
   "metadata": {},
   "source": [
    "Q.15 Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc7c08",
   "metadata": {},
   "source": [
    "Q.16 Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bae3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Feature Importances:', model.feature_importances_)\n",
    "\n",
    "plt.bar(range(X.shape[1]), model.feature_importances_)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb2e96",
   "metadata": {},
   "source": [
    "Q.17 Train a Gradient Boosting Regressor and evaluate using R-Squared Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('R-Squared Score:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04963e17",
   "metadata": {},
   "source": [
    "Q.18 Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "print('Gradient Boosting Accuracy:', accuracy_score(y_test, gb_pred))\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "print('XGBoost Accuracy:', accuracy_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b16478",
   "metadata": {},
   "source": [
    "Q.19 Train a CatBoost Classifier and evaluate using F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('F1 Score:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a7502",
   "metadata": {},
   "source": [
    "Q.20 Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46181dc",
   "metadata": {},
   "source": [
    "Q.21 Train an AdaBoost Classifier and visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.bar(range(X.shape[1]), model.feature_importances_)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c93c5c",
   "metadata": {},
   "source": [
    "Q.22 Train a Gradient Boosting Regressor and plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(GradientBoostingRegressor(), X, y, cv=5)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Score')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Validation Score')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf3e1c",
   "metadata": {},
   "source": [
    "Q.23 Train an XGBoost Classifier and visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa042ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c3820",
   "metadata": {},
   "source": [
    "Q.24 Train a CatBoost Classifier and plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20941633",
   "metadata": {},
   "source": [
    "Q.25 Train an AdaBoost Classifier with different numbers of estimators and compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [10, 50, 100, 200]:\n",
    "    model = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Number of Estimators: {n}, Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514c639",
   "metadata": {},
   "source": [
    "Q.26 Train a Gradient Boosting Classifier and visualize the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44e44f",
   "metadata": {},
   "source": [
    "Q.27 Train an XGBoost Regressor and tune the learning rate using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'learning_rate': [0.01, 0.1, 0.2, 0.3]}\n",
    "model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "grid = GridSearchCV(model, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid.best_params_)\n",
    "print('Best Score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e5f25",
   "metadata": {},
   "source": [
    "Q.28 Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Creating imbalanced dataset\n",
    "X_minority, y_minority = resample(X[y==1], y[y==1], n_samples=50, random_state=42)\n",
    "X_imbalanced = np.vstack((X[y==0], X_minority))\n",
    "y_imbalanced = np.hstack((y[y==0], y_minority))\n",
    "\n",
    "model = CatBoostClassifier(iterations=100, verbose=0, class_weights=[1, 10], random_state=42)\n",
    "model.fit(X_imbalanced, y_imbalanced)\n",
    "y_pred = model.predict(X_test)\n",
    "print('F1 Score with Class Weighting:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed174df",
   "metadata": {},
   "source": [
    "Q.29 Train an AdaBoost Classifier and analyze the effect of different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [0.01, 0.1, 0.5, 1]:\n",
    "    model = AdaBoostClassifier(n_estimators=100, learning_rate=lr, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Learning Rate: {lr}, Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fd4d4",
   "metadata": {},
   "source": [
    "Q.30 Train an XGBoost Classifier for multi-class classification and evaluate using log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "print('Log Loss:', log_loss(y_test, y_pred_proba))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
